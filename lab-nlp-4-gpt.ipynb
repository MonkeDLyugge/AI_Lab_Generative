{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Подключим необходимые библиотеки","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\n\nfrom datasets import load_dataset\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.model_selection import train_test_split\nimport nltk\n\nfrom collections import Counter\nfrom typing import List\nfrom tqdm import tqdm\n\nimport seaborn\nseaborn.set(palette='summer')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T18:51:07.693885Z","iopub.execute_input":"2024-05-27T18:51:07.694454Z","iopub.status.idle":"2024-05-27T18:51:13.344673Z","shell.execute_reply.started":"2024-05-27T18:51:07.694423Z","shell.execute_reply":"2024-05-27T18:51:13.343828Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:13.346519Z","iopub.execute_input":"2024-05-27T18:51:13.347043Z","iopub.status.idle":"2024-05-27T18:51:13.374286Z","shell.execute_reply.started":"2024-05-27T18:51:13.347014Z","shell.execute_reply":"2024-05-27T18:51:13.373261Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Загрузка датасета","metadata":{}},{"cell_type":"code","source":"conn = sqlite3.connect('../input/wikibooks.sqlite')\n\ndf = pd.read_sql_query(\"SELECT * FROM ru LIMIT 3300\", conn)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:13.375686Z","iopub.execute_input":"2024-05-27T18:51:13.376281Z","iopub.status.idle":"2024-05-27T18:51:15.504152Z","shell.execute_reply.started":"2024-05-27T18:51:13.376248Z","shell.execute_reply":"2024-05-27T18:51:15.503387Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sentences = []\n\nfor sentence in tqdm(df['body_text']):\n    sentences.extend(\n        [x.lower() for x in sent_tokenize(sentence, language='russian') if len(x) < 256]\n        )\n    \nprint(\"Количество предложений\", len(sentences))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:15.506102Z","iopub.execute_input":"2024-05-27T18:51:15.506458Z","iopub.status.idle":"2024-05-27T18:51:25.758070Z","shell.execute_reply.started":"2024-05-27T18:51:15.506429Z","shell.execute_reply":"2024-05-27T18:51:25.757158Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|██████████| 3300/3300 [00:10<00:00, 322.19it/s]","output_type":"stream"},{"name":"stdout","text":"Количество предложений 120873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"words = Counter()\n\nfor sentence in tqdm(sentences):\n    for word in nltk.word_tokenize(sentence):\n            words[word] += 1\n            \nvocab = set(['<unk>', '<bos>', '<eos>', '<pad>'])\nvocab_size = 20000\n\nfor elem in words.most_common(vocab_size):\n    vocab.add(elem[0])\n    \nprint(\"Всего слов в словаре:\", len(vocab))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:25.759238Z","iopub.execute_input":"2024-05-27T18:51:25.759541Z","iopub.status.idle":"2024-05-27T18:51:55.296492Z","shell.execute_reply.started":"2024-05-27T18:51:25.759504Z","shell.execute_reply":"2024-05-27T18:51:55.295543Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 120873/120873 [00:29<00:00, 4126.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Всего слов в словаре: 20004\n","output_type":"stream"}]},{"cell_type":"code","source":"word2ind = {char: i for i, char in enumerate(vocab)}\nind2word = {i: char for char, i in word2ind.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.297746Z","iopub.execute_input":"2024-05-27T18:51:55.298100Z","iopub.status.idle":"2024-05-27T18:51:55.309250Z","shell.execute_reply.started":"2024-05-27T18:51:55.298063Z","shell.execute_reply":"2024-05-27T18:51:55.308379Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, sheduler = None):\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n    losses = []\n    perplexity = []\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        logits = model(batch['input_ids'])\n        loss = criterion(\n            logits, batch['target_ids'].flatten())\n        loss.backward()\n        optimizer.step()\n        \n        perplexity.append(torch.exp(loss).item())\n        losses.append(loss.item())\n\n    perplexity = sum(perplexity) / len(perplexity)\n    losses = sum(losses) / len(losses)    \n    return perplexity, losses\n\n\n\ndef eval_epoch(model, val_loader, criterion):\n    model.eval()\n    perplexity = []\n    losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            logits = model(batch['input_ids'])\n            loss = criterion(\n                logits,\n                batch['target_ids'].flatten()\n                )\n            perplexity.append(torch.exp(loss).item())\n            losses.append(loss.item())\n\n    perplexity = sum(perplexity) / len(perplexity)\n    losses = sum(losses) / len(losses)\n    return perplexity, losses\n\n\n\ndef train(train_dataloader, eval_dataloader, model, epochs, ignore_index = word2ind['<pad>'] ,\n          optimizer=None, criterion=None, sheduler=None):\n\n    if optimizer is None:\n      optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    if criterion is None:\n      criterion = nn.CrossEntropyLoss(ignore_index=ignore_index).to(device)\n    \n    min_lr = 1e-4\n    initial_lr = 3e-4\n    lambda_func = lambda epoch: max(0.99 ** epoch, min_lr / initial_lr)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n\n    best_model_wts = model.state_dict()\n    best_perplexity = 10e10\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_perplexirty {t_acc:0.4f} val_perplexirty {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n\n        for epoch in range(epochs):\n            train_perplexirty, train_loss = fit_epoch(model, train_dataloader, criterion, optimizer)\n            scheduler.step()\n\n            val_perplexirty, val_loss = eval_epoch(model, eval_dataloader, criterion)\n            history.append((train_loss, train_perplexirty, val_loss, val_perplexirty))\n            if val_perplexirty < best_perplexity:\n                best_perplexity = val_perplexirty\n                best_model_wts = model.state_dict()\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_perplexirty, v_acc=val_perplexirty))\n\n    print('Best val perplexirty: {:4f}'.format(best_perplexity))\n    model.load_state_dict(best_model_wts)\n\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.310572Z","iopub.execute_input":"2024-05-27T18:51:55.310873Z","iopub.status.idle":"2024-05-27T18:51:55.327558Z","shell.execute_reply.started":"2024-05-27T18:51:55.310848Z","shell.execute_reply":"2024-05-27T18:51:55.326771Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Функции необходимые при обучении/загрузке датасета/генерации текста","metadata":{}},{"cell_type":"code","source":"class WordDataset(torch.utils.data.Dataset):\n    def __init__(self, sentences, word2ind):\n        super().__init__()\n        self.data = sentences\n        self.word2ind = word2ind\n        self.unk_id = self.word2ind['<unk>']\n        self.bos_id = self.word2ind['<bos>']\n        self.eos_id = self.word2ind['<eos>']\n        self.pad_id = self.word2ind['<pad>']\n\n    def __getitem__(self, idx: int) -> List[int]:\n        tokenized_sentence = [self.bos_id]\n        tokenized_sentence += self.data[idx]\n        tokenized_sentence += [self.eos_id]\n        \n        return tokenized_sentence\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \n    \n    \ndef collate_fn_with_padding(\n    input_batch: List[List[int]], pad_id=word2ind['<pad>'], max_seq_len: int=96) -> torch.Tensor:\n\n    new_batch = []\n    for sequence in input_batch:\n        if len(sequence) > max_seq_len:\n            sequence = sequence[:max_seq_len - 1] + [sequence[-1]]\n        else:\n            for _ in range(max_seq_len - len(sequence)):\n                sequence.append(pad_id)\n        new_batch.append(sequence)\n\n    sequences = torch.LongTensor(new_batch).to(device)\n\n    new_batch = {\n        'input_ids': sequences[:,:-1],\n        'target_ids': sequences[:,1:]\n    }\n\n    return new_batch\n\ndef generate_sequence(model, dict_2ind ,ind2dict, starting_seq: int, max_seq_len: int = 256) -> str:\n    device = 'cpu'\n    model = model.to(device)\n    \n    idx = torch.zeros((1,1), dtype=torch.long).to(device)\n    idx[0, 0] = starting_seq\n    \n    block_size = 256\n\n    model.eval()\n    t = idx.shape[1]\n    with torch.no_grad():\n        for i in range(max_seq_len):\n            idx_cond = idx[:, -block_size:]\n            logits = model.forward(idx_cond)\n            logits = logits.reshape(1, t, -1)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n            if t < block_size:\n                t += 1\n                \n            if idx_next.item() == dict_2ind['<eos>']:\n                break\n\n    words = ' '.join([ind2dict[i.item()] for i in idx[0]])\n\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:16:44.063816Z","iopub.execute_input":"2024-05-27T21:16:44.064196Z","iopub.status.idle":"2024-05-27T21:16:44.080003Z","shell.execute_reply.started":"2024-05-27T21:16:44.064159Z","shell.execute_reply":"2024-05-27T21:16:44.079112Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# Main Model","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(\n            self, \n            num_heads: int, \n            n_embed: int, \n            block_size: int\n        ):\n        super(TransformerBlock, self).__init__()\n        hidden_dim = n_embed // num_heads\n        self.mhsa = MultiHeadSelfAttention(num_heads, hidden_dim, n_embed, block_size)\n        self.feed_forward = FeedForward(n_embed)\n        self.norm1 = nn.LayerNorm(n_embed)\n        self.norm2 = nn.LayerNorm(n_embed)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.mhsa(self.norm1(x))\n        x = x + self.feed_forward(self.norm2(x))\n        return x\n\n\nclass FeedForward(nn.Module):\n    def __init__(\n            self, \n            n_embed: int, \n            extend_width: int=4, \n            dropout: float=0.2\n        ):\n        super(FeedForward, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Linear(n_embed, extend_width*n_embed), \n            nn.ReLU(),\n            nn.Linear(extend_width*n_embed, n_embed), \n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.layer(x)\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(\n            self, \n            num_heads: int, \n            hidden_dim: int, \n            n_embed: int, \n            block_size: int, \n            dropout: float=0.2\n        ):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.num_heads = num_heads\n        self.heads = nn.ModuleList([SingleHead(hidden_dim, n_embed, block_size) for _ in range(self.num_heads)])\n        self.project = nn.Linear(n_embed, n_embed)\n        self.drop = nn.Dropout(dropout)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = torch.cat([sh(x) for sh in self.heads], dim=-1)\n        out = self.project(out)\n        out = self.drop(out)\n        return out\n\n\nclass SingleHead(nn.Module):\n    def __init__(\n            self, \n            hidden_dim: int, \n            n_embed: int, \n            block_size: int, \n            dropout: float=0.2\n        ):\n        super(SingleHead, self).__init__()\n        self.key = nn.Linear(n_embed, hidden_dim, bias=False)\n        self.query = nn.Linear(n_embed, hidden_dim, bias=False)\n        self.value = nn.Linear(n_embed, hidden_dim, bias=False)\n        self.drop = nn.Dropout(dropout)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        weights = q @ k.transpose(-2, -1) * C**(-0.5)\n        masked_weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        masked_probs = F.softmax(masked_weights, dim=-1)\n        masked_probs = self.drop(masked_probs)\n        v = self.value(x)\n        out = masked_probs @ v\n        return out\n\n\nclass GPT(nn.Module):\n    def __init__(\n            self, \n            vocab_size: int, \n            block_size: int, \n            n_embed: int, \n            num_heads: int, \n            n_layers: int\n        ):\n        super(GPT, self).__init__()\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        self.embedding = nn.Embedding(vocab_size, n_embed)\n        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n        self.blocks = nn.Sequential(\n            *[TransformerBlock(num_heads, n_embed, block_size) for _ in range(n_layers)],\n        )\n        self.norm = nn.LayerNorm(n_embed)        \n        self.fc = nn.Linear(n_embed, vocab_size)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, T = x.shape\n        token_embeddings = self.embedding(x) # B, T -> B, T, N_EMB\n        positional_embedding = self.positional_embedding_table(torch.arange(T, device=x.device)) # T -> T, C\n        token_embeddings = token_embeddings + positional_embedding # B, T, C + T, C -> B, T, C\n        blocks_out = self.blocks(token_embeddings)\n        blocks_out = self.norm(blocks_out)\n        logits = self.fc(blocks_out) # B, T, N_EMB -> B, T, C\n        logits = logits.reshape(B*T, self.vocab_size)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.345905Z","iopub.execute_input":"2024-05-27T18:51:55.346235Z","iopub.status.idle":"2024-05-27T18:51:55.371050Z","shell.execute_reply.started":"2024-05-27T18:51:55.346205Z","shell.execute_reply":"2024-05-27T18:51:55.370280Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.373943Z","iopub.execute_input":"2024-05-27T18:51:55.374216Z","iopub.status.idle":"2024-05-27T18:51:55.420360Z","shell.execute_reply.started":"2024-05-27T18:51:55.374193Z","shell.execute_reply":"2024-05-27T18:51:55.419656Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def sentence_pre(s):\n    return [word2ind.get(w, word2ind['<unk>']) for w in nltk.word_tokenize(s)]","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.421381Z","iopub.execute_input":"2024-05-27T18:51:55.421642Z","iopub.status.idle":"2024-05-27T18:51:55.427328Z","shell.execute_reply.started":"2024-05-27T18:51:55.421619Z","shell.execute_reply":"2024-05-27T18:51:55.426317Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_sentences = list(map(sentence_pre, train_sentences))\neval_sentences = list(map(sentence_pre, eval_sentences))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:51:55.428611Z","iopub.execute_input":"2024-05-27T18:51:55.428966Z","iopub.status.idle":"2024-05-27T18:52:24.356606Z","shell.execute_reply.started":"2024-05-27T18:51:55.428936Z","shell.execute_reply":"2024-05-27T18:52:24.355847Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset = WordDataset(train_sentences, word2ind)\neval_dataset = WordDataset(eval_sentences, word2ind)\ntrain_dataloader = DataLoader(\n    train_dataset, collate_fn=collate_fn_with_padding, batch_size=128, shuffle=True, num_workers=0)\n\neval_dataloader = DataLoader(\n    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=128, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:52:24.357661Z","iopub.execute_input":"2024-05-27T18:52:24.357942Z","iopub.status.idle":"2024-05-27T18:52:24.363175Z","shell.execute_reply.started":"2024-05-27T18:52:24.357916Z","shell.execute_reply":"2024-05-27T18:52:24.362335Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nblock_size = 256\nn_embed = 384\nnum_heads = 6\nn_layers = 6","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:52:24.364313Z","iopub.execute_input":"2024-05-27T18:52:24.364580Z","iopub.status.idle":"2024-05-27T18:52:24.374963Z","shell.execute_reply.started":"2024-05-27T18:52:24.364557Z","shell.execute_reply":"2024-05-27T18:52:24.374216Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = GPT(vocab_size=vocab_size, block_size=block_size, n_embed=n_embed, num_heads=num_heads, n_layers=n_layers).to(device)\n\nnum_params = sum(p.numel() for p in model.parameters())\nprint(model)\nprint(f\"Number of model parameters: {num_params:,}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:52:24.376102Z","iopub.execute_input":"2024-05-27T18:52:24.376974Z","iopub.status.idle":"2024-05-27T18:52:24.910756Z","shell.execute_reply.started":"2024-05-27T18:52:24.376942Z","shell.execute_reply":"2024-05-27T18:52:24.909866Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"GPT(\n  (embedding): Embedding(20004, 384)\n  (positional_embedding_table): Embedding(256, 384)\n  (blocks): Sequential(\n    (0): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): TransformerBlock(\n      (mhsa): MultiHeadSelfAttention(\n        (heads): ModuleList(\n          (0-5): 6 x SingleHead(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (drop): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (project): Linear(in_features=384, out_features=384, bias=True)\n        (drop): Dropout(p=0.2, inplace=False)\n      )\n      (feed_forward): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (fc): Linear(in_features=384, out_features=20004, bias=True)\n)\nNumber of model parameters: 26,122,020\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model, losses = train(train_dataloader, eval_dataloader, model, 30, ignore_index = word2ind[\"<pad>\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:52:24.911900Z","iopub.execute_input":"2024-05-27T18:52:24.912174Z","iopub.status.idle":"2024-05-27T20:54:30.786360Z","shell.execute_reply.started":"2024-05-27T18:52:24.912151Z","shell.execute_reply":"2024-05-27T20:54:30.785414Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"epoch:   3%|▎         | 1/30 [04:05<1:58:27, 245.07s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 001 train_loss: 5.7074     val_loss 5.1998 train_perplexirty 389.2737 val_perplexirty 182.2926\n","output_type":"stream"},{"name":"stderr","text":"epoch:   7%|▋         | 2/30 [08:09<1:54:08, 244.59s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 002 train_loss: 4.9312     val_loss 4.8145 train_perplexirty 140.1134 val_perplexirty 124.0320\n","output_type":"stream"},{"name":"stderr","text":"epoch:  10%|█         | 3/30 [12:13<1:49:59, 244.42s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 003 train_loss: 4.5089     val_loss 4.6039 train_perplexirty 91.4058 val_perplexirty 100.4387\n","output_type":"stream"},{"name":"stderr","text":"epoch:  13%|█▎        | 4/30 [16:17<1:45:52, 244.33s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 004 train_loss: 4.1919     val_loss 4.4881 train_perplexirty 66.5211 val_perplexirty 89.4769\n","output_type":"stream"},{"name":"stderr","text":"epoch:  17%|█▋        | 5/30 [20:21<1:41:46, 244.25s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 005 train_loss: 3.9345     val_loss 4.4217 train_perplexirty 51.4186 val_perplexirty 83.7435\n","output_type":"stream"},{"name":"stderr","text":"epoch:  20%|██        | 6/30 [24:25<1:37:40, 244.20s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 006 train_loss: 3.7136     val_loss 4.4016 train_perplexirty 41.2362 val_perplexirty 82.0832\n","output_type":"stream"},{"name":"stderr","text":"epoch:  23%|██▎       | 7/30 [28:30<1:33:35, 244.16s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 007 train_loss: 3.5176     val_loss 4.4011 train_perplexirty 33.9049 val_perplexirty 82.0706\n","output_type":"stream"},{"name":"stderr","text":"epoch:  27%|██▋       | 8/30 [32:34<1:29:30, 244.13s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 008 train_loss: 3.3398     val_loss 4.4256 train_perplexirty 28.3729 val_perplexirty 84.1377\n","output_type":"stream"},{"name":"stderr","text":"epoch:  30%|███       | 9/30 [36:38<1:25:26, 244.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 009 train_loss: 3.1802     val_loss 4.4597 train_perplexirty 24.2117 val_perplexirty 87.0989\n","output_type":"stream"},{"name":"stderr","text":"epoch:  33%|███▎      | 10/30 [40:42<1:21:22, 244.11s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 010 train_loss: 3.0330     val_loss 4.5091 train_perplexirty 20.8765 val_perplexirty 91.5540\n","output_type":"stream"},{"name":"stderr","text":"epoch:  37%|███▋      | 11/30 [44:46<1:17:17, 244.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 011 train_loss: 2.9007     val_loss 4.5579 train_perplexirty 18.3062 val_perplexirty 96.1564\n","output_type":"stream"},{"name":"stderr","text":"epoch:  40%|████      | 12/30 [48:50<1:13:13, 244.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 012 train_loss: 2.7808     val_loss 4.6251 train_perplexirty 16.2398 val_perplexirty 102.9171\n","output_type":"stream"},{"name":"stderr","text":"epoch:  43%|████▎     | 13/30 [52:54<1:09:09, 244.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 013 train_loss: 2.6690     val_loss 4.6861 train_perplexirty 14.5097 val_perplexirty 109.4391\n","output_type":"stream"},{"name":"stderr","text":"epoch:  47%|████▋     | 14/30 [56:58<1:05:05, 244.07s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 014 train_loss: 2.5688     val_loss 4.7498 train_perplexirty 13.1302 val_perplexirty 116.7037\n","output_type":"stream"},{"name":"stderr","text":"epoch:  50%|█████     | 15/30 [1:01:02<1:01:01, 244.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 015 train_loss: 2.4767     val_loss 4.8227 train_perplexirty 11.9716 val_perplexirty 125.6011\n","output_type":"stream"},{"name":"stderr","text":"epoch:  53%|█████▎    | 16/30 [1:05:06<56:56, 244.07s/it]  ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 016 train_loss: 2.3929     val_loss 4.8815 train_perplexirty 11.0103 val_perplexirty 133.2416\n","output_type":"stream"},{"name":"stderr","text":"epoch:  57%|█████▋    | 17/30 [1:09:10<52:53, 244.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 017 train_loss: 2.3170     val_loss 4.9483 train_perplexirty 10.2079 val_perplexirty 142.5939\n","output_type":"stream"},{"name":"stderr","text":"epoch:  60%|██████    | 18/30 [1:13:14<48:48, 244.06s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 018 train_loss: 2.2474     val_loss 5.0167 train_perplexirty 9.5149 val_perplexirty 152.7303\n","output_type":"stream"},{"name":"stderr","text":"epoch:  63%|██████▎   | 19/30 [1:17:18<44:44, 244.04s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 019 train_loss: 2.1853     val_loss 5.0792 train_perplexirty 8.9442 val_perplexirty 162.6668\n","output_type":"stream"},{"name":"stderr","text":"epoch:  67%|██████▋   | 20/30 [1:21:22<40:40, 244.06s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 020 train_loss: 2.1258     val_loss 5.1391 train_perplexirty 8.4245 val_perplexirty 172.7972\n","output_type":"stream"},{"name":"stderr","text":"epoch:  70%|███████   | 21/30 [1:25:26<36:36, 244.06s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 021 train_loss: 2.0700     val_loss 5.2038 train_perplexirty 7.9666 val_perplexirty 184.4470\n","output_type":"stream"},{"name":"stderr","text":"epoch:  73%|███████▎  | 22/30 [1:29:30<32:32, 244.04s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 022 train_loss: 2.0205     val_loss 5.2703 train_perplexirty 7.5809 val_perplexirty 197.2539\n","output_type":"stream"},{"name":"stderr","text":"epoch:  77%|███████▋  | 23/30 [1:33:35<28:28, 244.07s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 023 train_loss: 1.9754     val_loss 5.3235 train_perplexirty 7.2453 val_perplexirty 208.2208\n","output_type":"stream"},{"name":"stderr","text":"epoch:  80%|████████  | 24/30 [1:37:39<24:24, 244.08s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 024 train_loss: 1.9316     val_loss 5.3808 train_perplexirty 6.9336 val_perplexirty 220.5924\n","output_type":"stream"},{"name":"stderr","text":"epoch:  83%|████████▎ | 25/30 [1:41:43<20:20, 244.15s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 025 train_loss: 1.8909     val_loss 5.4325 train_perplexirty 6.6551 val_perplexirty 232.4053\n","output_type":"stream"},{"name":"stderr","text":"epoch:  87%|████████▋ | 26/30 [1:45:47<16:16, 244.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 026 train_loss: 1.8539     val_loss 5.4875 train_perplexirty 6.4134 val_perplexirty 245.7721\n","output_type":"stream"},{"name":"stderr","text":"epoch:  90%|█████████ | 27/30 [1:49:51<12:12, 244.14s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 027 train_loss: 1.8194     val_loss 5.5330 train_perplexirty 6.1959 val_perplexirty 257.2050\n","output_type":"stream"},{"name":"stderr","text":"epoch:  93%|█████████▎| 28/30 [1:53:55<08:08, 244.16s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 028 train_loss: 1.7853     val_loss 5.5884 train_perplexirty 5.9880 val_perplexirty 272.1051\n","output_type":"stream"},{"name":"stderr","text":"epoch:  97%|█████████▋| 29/30 [1:58:00<04:04, 244.13s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 029 train_loss: 1.7539     val_loss 5.6389 train_perplexirty 5.8000 val_perplexirty 286.3192\n","output_type":"stream"},{"name":"stderr","text":"epoch: 100%|██████████| 30/30 [2:02:04<00:00, 244.13s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 030 train_loss: 1.7252     val_loss 5.6779 train_perplexirty 5.6343 val_perplexirty 297.6578\nBest val perplexirty: 82.070595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(best_model.state_dict(), \"best_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:16:49.249149Z","iopub.execute_input":"2024-05-27T21:16:49.249948Z","iopub.status.idle":"2024-05-27T21:16:49.479558Z","shell.execute_reply.started":"2024-05-27T21:16:49.249916Z","shell.execute_reply":"2024-05-27T21:16:49.478551Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"generate_sequence(best_model, word2ind, ind2word,starting_seq=word2ind['облако'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T21:17:55.904872Z","iopub.execute_input":"2024-05-27T21:17:55.905577Z","iopub.status.idle":"2024-05-27T21:17:56.741672Z","shell.execute_reply.started":"2024-05-27T21:17:55.905543Z","shell.execute_reply":"2024-05-27T21:17:56.740766Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"'облако 1 ) наличие <unk> в нарушений восприятия ; 2 ) <unk> <unk> в объеме ; 3 ) наличие хронического <unk> крови ; 4 ) увеличение ожидаемой средней продолжительности жизни ; 29 . <eos>'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Все выводы по работе с GPT представлены в отчете","metadata":{}}]}