{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Подключим необходимые библиотеки","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\n\nfrom datasets import load_dataset\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.model_selection import train_test_split\nimport nltk\n\nfrom collections import Counter\nfrom typing import List\nfrom tqdm import tqdm\n\nimport seaborn\nseaborn.set(palette='summer')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T08:02:19.034566Z","iopub.execute_input":"2024-05-27T08:02:19.034911Z","iopub.status.idle":"2024-05-27T08:02:25.404534Z","shell.execute_reply.started":"2024-05-27T08:02:19.034883Z","shell.execute_reply":"2024-05-27T08:02:25.403529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:02:25.406496Z","iopub.execute_input":"2024-05-27T08:02:25.407080Z","iopub.status.idle":"2024-05-27T08:02:25.439306Z","shell.execute_reply.started":"2024-05-27T08:02:25.407026Z","shell.execute_reply":"2024-05-27T08:02:25.438367Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Загрузка датасета","metadata":{}},{"cell_type":"code","source":"conn = sqlite3.connect('/kaggle/input/wikibooks-dataset/wikibooks.sqlite')\n\ndf = pd.read_sql_query(\"SELECT * FROM ru LIMIT 3300\", conn)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:02:25.440594Z","iopub.execute_input":"2024-05-27T08:02:25.441146Z","iopub.status.idle":"2024-05-27T08:02:27.674657Z","shell.execute_reply.started":"2024-05-27T08:02:25.441115Z","shell.execute_reply":"2024-05-27T08:02:27.673847Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sentences = []\n\nfor sentence in tqdm(df['body_text']):\n    sentences.extend(\n        [x.lower() for x in sent_tokenize(sentence, language='russian') if len(x) < 256]\n        )\n    \nprint(\"Количество предложений\", len(sentences))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:02:27.676595Z","iopub.execute_input":"2024-05-27T08:02:27.676884Z","iopub.status.idle":"2024-05-27T08:02:37.924493Z","shell.execute_reply.started":"2024-05-27T08:02:27.676860Z","shell.execute_reply":"2024-05-27T08:02:37.923576Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|██████████| 3300/3300 [00:10<00:00, 322.32it/s]","output_type":"stream"},{"name":"stdout","text":"Количество предложений 120873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, sheduler = None):\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n    losses = []\n    perplexity = []\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n        loss = criterion(\n            logits, batch['target_ids'].flatten())\n        loss.backward()\n        optimizer.step()\n        \n        perplexity.append(torch.exp(loss).item())\n        losses.append(loss.item())\n        \n    perplexity = sum(perplexity) / len(perplexity)\n    losses = sum(losses) / len(losses)    \n    return perplexity, losses\n\n\n\ndef eval_epoch(model, val_loader, criterion):\n    model.eval()\n    perplexity = []\n    losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n            loss = criterion(\n                logits,\n                batch['target_ids'].flatten()\n                )\n            perplexity.append(torch.exp(loss).item())\n            losses.append(loss.item())\n\n    perplexity = sum(perplexity) / len(perplexity)\n    losses = sum(losses) / len(losses)\n    return perplexity, losses\n\n\n\ndef train(train_dataloader, eval_dataloader, model, epochs, ignore_index = word2ind['<pad>'] ,\n          optimizer=None, criterion=None, sheduler=None):\n\n    if optimizer is None:\n      optimizer = torch.optim.Adam(model.parameters())\n\n    if criterion is None:\n      criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    best_model_wts = model.state_dict()\n    best_perplexity = 10e10\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_perplexirty {t_acc:0.4f} val_perplexirty {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n\n        for epoch in range(epochs):\n            train_perplexirty, train_loss = fit_epoch(model, train_dataloader, criterion, optimizer)\n\n            val_perplexirty, val_loss = eval_epoch(model, eval_dataloader, criterion)\n            history.append((train_loss, train_perplexirty, val_loss, val_perplexirty))\n            if val_perplexirty < best_perplexity:\n                best_perplexity = val_perplexirty\n                best_model_wts = model.state_dict()\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_perplexirty, v_acc=val_perplexirty))\n\n    print('Best val perplexirty: {:4f}'.format(best_perplexity))\n    model.load_state_dict(best_model_wts)\n\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:03:53.281254Z","iopub.execute_input":"2024-05-27T08:03:53.281940Z","iopub.status.idle":"2024-05-27T08:03:53.297773Z","shell.execute_reply.started":"2024-05-27T08:03:53.281909Z","shell.execute_reply":"2024-05-27T08:03:53.296704Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Функции необходимые при обучении/загрузке датасета/генерации текста","metadata":{}},{"cell_type":"code","source":"class WordDataset:\n    def __init__(self, sentences):\n        self.data = sentences\n        self.unk_id = word2ind['<unk>']\n        self.bos_id = word2ind['<bos>']\n        self.eos_id = word2ind['<eos>']\n        self.pad_id = word2ind['<pad>']\n\n    def __getitem__(self, idx: int) -> List[int]:\n        tokenized_sentence = [self.bos_id]\n        tokenized_sentence += [word2ind.get(word, self.unk_id) for word in nltk.word_tokenize(self.data[idx])]\n        tokenized_sentence += [self.eos_id]\n        \n        return tokenized_sentence\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \n    \n    \ndef collate_fn_with_padding(\n    input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> torch.Tensor:\n    seq_lens = [len(x) for x in input_batch]\n    max_seq_len = max(seq_lens)\n\n    new_batch = []\n    for sequence in input_batch:\n        for _ in range(max_seq_len - len(sequence)):\n            sequence.append(pad_id)\n        new_batch.append(sequence)\n\n    sequences = torch.LongTensor(new_batch).to(device)\n\n    new_batch = {\n        'input_ids': sequences[:,:-1],\n        'target_ids': sequences[:,1:]\n    }\n\n    return new_batch\n\ndef generate_sequence(model, dict_2ind ,ind2dict, starting_seq: str, max_seq_len: int = 256) -> str:\n    device = 'cpu'\n    model = model.to(device)\n    input_ids = [dict_2ind['<bos>']] + [\n        dict_2ind.get(char, dict_2ind['<unk>']) for char in starting_seq]\n    input_ids = torch.LongTensor(input_ids).to(device)\n\n    model.eval()\n    with torch.no_grad():\n        for i in range(max_seq_len):\n            next_char_distribution = model(input_ids)[-1]\n            next_char = next_char_distribution.squeeze().argmax()\n            input_ids = torch.cat([input_ids, next_char.unsqueeze(0)])\n\n            if next_char.item() == dict_2ind['<eos>']:\n                break\n\n    words = ' '.join([ind2dict[idx.item()] for idx in input_ids])\n\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:03:54.153996Z","iopub.execute_input":"2024-05-27T08:03:54.154377Z","iopub.status.idle":"2024-05-27T08:03:54.168526Z","shell.execute_reply.started":"2024-05-27T08:03:54.154347Z","shell.execute_reply":"2024-05-27T08:03:54.167505Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Main Model","metadata":{}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    def __init__(self, vocab_size, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        \n        self.lstm_1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional = True)\n        self.lstm_2 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional = True)\n        self.lstm_3 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional = True)\n        self.lstm_4 = nn.LSTM(hidden_dim*2, hidden_dim, batch_first=True, bidirectional = True)\n            \n        self.linear = nn.Linear(hidden_dim*2, hidden_dim)\n        self.projection = nn.Linear(hidden_dim, vocab_size)\n\n        self.non_lin = nn.Tanh()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n        embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\n        output, _ = self.lstm_1(embeddings)\n        output, _ = self.lstm_2(output)\n        output, _ = self.lstm_3(output)\n        output, _ = self.lstm_4(output)\n        output = self.dropout(self.linear(self.non_lin(output)))  # [batch_size, seq_len, hidden_dim]\n        projection = self.projection(self.non_lin(output))  # [batch_size, seq_len, vocab_size]\n\n        return projection","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:12:24.210610Z","iopub.execute_input":"2024-05-27T08:12:24.211157Z","iopub.status.idle":"2024-05-27T08:12:24.222040Z","shell.execute_reply.started":"2024-05-27T08:12:24.211126Z","shell.execute_reply":"2024-05-27T08:12:24.221034Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"words = Counter()\n\nfor sentence in tqdm(sentences):\n    for word in nltk.word_tokenize(sentence):\n            words[word] += 1\n            \nvocab = set(['<unk>', '<bos>', '<eos>', '<pad>'])\nvocab_size = 40000\n\nfor elem in words.most_common(vocab_size):\n    vocab.add(elem[0])\n    \nprint(\"Всего слов в словаре:\", len(vocab))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:11:14.032742Z","iopub.execute_input":"2024-05-27T08:11:14.033265Z","iopub.status.idle":"2024-05-27T08:11:43.125402Z","shell.execute_reply.started":"2024-05-27T08:11:14.033238Z","shell.execute_reply":"2024-05-27T08:11:43.124486Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"100%|██████████| 120873/120873 [00:28<00:00, 4174.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Всего слов в словаре: 40004\n","output_type":"stream"}]},{"cell_type":"code","source":"word2ind = {char: i for i, char in enumerate(vocab)}\nind2word = {i: char for char, i in word2ind.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:11:43.127132Z","iopub.execute_input":"2024-05-27T08:11:43.128034Z","iopub.status.idle":"2024-05-27T08:11:43.151741Z","shell.execute_reply.started":"2024-05-27T08:11:43.127995Z","shell.execute_reply":"2024-05-27T08:11:43.150823Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)\n\ntrain_dataset = WordDataset(train_sentences)\neval_dataset = WordDataset(eval_sentences)\n\ntrain_dataloader = DataLoader(\n    train_dataset, collate_fn=collate_fn_with_padding, batch_size=64,)\n\neval_dataloader = DataLoader(\n    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:11:43.152952Z","iopub.execute_input":"2024-05-27T08:11:43.153533Z","iopub.status.idle":"2024-05-27T08:11:43.197936Z","shell.execute_reply.started":"2024-05-27T08:11:43.153501Z","shell.execute_reply":"2024-05-27T08:11:43.197108Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(hidden_dim=256, vocab_size=len(vocab)).to(device)\n\nnum_params = sum(p.numel() for p in model.parameters())\nprint(model)\nprint(f\"Number of model parameters: {num_params:,}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:12:27.843820Z","iopub.execute_input":"2024-05-27T08:12:27.844291Z","iopub.status.idle":"2024-05-27T08:12:28.129201Z","shell.execute_reply.started":"2024-05-27T08:12:27.844256Z","shell.execute_reply":"2024-05-27T08:12:28.128278Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"LanguageModel(\n  (embedding): Embedding(40004, 256)\n  (lstm_1): LSTM(256, 256, batch_first=True, bidirectional=True)\n  (lstm_2): LSTM(512, 256, batch_first=True, bidirectional=True)\n  (lstm_3): LSTM(512, 256, batch_first=True, bidirectional=True)\n  (lstm_4): LSTM(512, 256, batch_first=True, bidirectional=True)\n  (linear): Linear(in_features=512, out_features=256, bias=True)\n  (projection): Linear(in_features=256, out_features=40004, bias=True)\n  (non_lin): Tanh()\n  (dropout): Dropout(p=0.2, inplace=False)\n)\nNumber of model parameters: 26,436,932\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model, losses = train(train_dataloader, eval_dataloader, model, 10, ignore_index = word2ind[\"<pad>\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:12:30.543908Z","iopub.execute_input":"2024-05-27T08:12:30.544725Z","iopub.status.idle":"2024-05-27T08:40:11.378810Z","shell.execute_reply.started":"2024-05-27T08:12:30.544696Z","shell.execute_reply":"2024-05-27T08:40:11.377791Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"epoch:  10%|█         | 1/10 [02:46<24:56, 166.31s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 001 train_loss: 6.2165     val_loss 4.4502 train_perplexirty 950.2023 val_perplexirty 86.3828\n","output_type":"stream"},{"name":"stderr","text":"epoch:  20%|██        | 2/10 [05:32<22:08, 166.07s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 002 train_loss: 3.6826     val_loss 3.0068 train_perplexirty 43.8529 val_perplexirty 20.3645\n","output_type":"stream"},{"name":"stderr","text":"epoch:  30%|███       | 3/10 [08:18<19:21, 165.97s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 003 train_loss: 2.6966     val_loss 2.2747 train_perplexirty 15.2892 val_perplexirty 9.7852\n","output_type":"stream"},{"name":"stderr","text":"epoch:  40%|████      | 4/10 [11:03<16:35, 165.89s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 004 train_loss: 1.9999     val_loss 1.7284 train_perplexirty 7.5591 val_perplexirty 5.6607\n","output_type":"stream"},{"name":"stderr","text":"epoch:  50%|█████     | 5/10 [13:49<13:48, 165.78s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 005 train_loss: 1.5587     val_loss 1.4527 train_perplexirty 4.7937 val_perplexirty 4.2946\n","output_type":"stream"},{"name":"stderr","text":"epoch:  60%|██████    | 6/10 [16:35<11:03, 165.76s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 006 train_loss: 1.2813     val_loss 1.2659 train_perplexirty 3.6192 val_perplexirty 3.5613\n","output_type":"stream"},{"name":"stderr","text":"epoch:  70%|███████   | 7/10 [19:20<08:17, 165.76s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 007 train_loss: 1.0773     val_loss 1.1091 train_perplexirty 2.9480 val_perplexirty 3.0433\n","output_type":"stream"},{"name":"stderr","text":"epoch:  80%|████████  | 8/10 [22:07<05:32, 166.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 008 train_loss: 0.8751     val_loss 0.9516 train_perplexirty 2.4070 val_perplexirty 2.5987\n","output_type":"stream"},{"name":"stderr","text":"epoch:  90%|█████████ | 9/10 [24:54<02:46, 166.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 009 train_loss: 0.7142     val_loss 0.8469 train_perplexirty 2.0487 val_perplexirty 2.3398\n","output_type":"stream"},{"name":"stderr","text":"epoch: 100%|██████████| 10/10 [27:40<00:00, 166.08s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 010 train_loss: 0.5945     val_loss 0.7705 train_perplexirty 1.8150 val_perplexirty 2.1671\nBest val perplexirty: 2.167140\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"generate_sequence(model, word2ind, ind2word,starting_seq=nltk.word_tokenize('империя'))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:45:34.688567Z","iopub.execute_input":"2024-05-27T08:45:34.688893Z","iopub.status.idle":"2024-05-27T08:45:34.728929Z","shell.execute_reply.started":"2024-05-27T08:45:34.688870Z","shell.execute_reply":"2024-05-27T08:45:34.728045Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"'<bos> империя чистый англ <eos>'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Все выводы по работе с RNN представлены в отчете","metadata":{}}]}