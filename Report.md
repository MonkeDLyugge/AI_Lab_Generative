## Отчёт по лабораторной работе

Выполнили:


Студент (ФИО) | Роль в проекте   | Оценка
-------------|---------------------|------
Люгге Томас Валерьевич | обучал все RNN, выбирал датасет, писал отчет |  

### Отчет работы с RNN

В ходе работы были рассмотрены следующие архитектуры:

1. Simple RNN с посимвольной и по-словной токенизацией.
2. Однонаправленная однослойная и многослойная LSTM с посимвольной токенизацией, токенизацией по словам.
3. Двунаправленная LSTM.

#### Simple RNN

GRU продемонстрировала базовую способность к генерации текста, GRU не имеет long памяти, как у LSTM (long short-term memory), поэтому на этапе создании модели было ясно, что качество генерации на больших последовательностях будет храмать очень сильно, а на маленьких последовательностях показывать приемлемые результаты. При посимвольной токенизации модель показала, как было сказано ранее, приемлимые результаты при генерации небольшого текста (2-4 слова). По-словная токенизация улучшила семантическую целостность текста, но остались такие же проблемы как и при посимвольной токенизации.

Плюсы:
- Простота реализации и быстрая скорость обучения на коротких текстах.
- Меньшая вычислительная сложность по сравнению с более сложными архитектурами.

Минусы:
- Плохая работа с длинными зависимостями.
- Посимвольная токенизация может приводить к потере смысла текста.

#### Однонаправленная LSTM

Однослойная LSTM с посимвольной токенизацией продемонстрировала улучшенные результаты по сравнению с GRU. В реализации LSTM имеется компоненты как краткосрочной памяти, так и долгосрочной, но этого недостаточно, чтобы запоминать длинные тексты. Модель была способна лучше сохранять контекст на больших последовательностях. Токенизация по словам позволила дополнительно улучшить качество генерируемого текста, так как модели легче работать с целыми словами и их частями, чем с отдельными символами.

Многослойная LSTM усилила эту тенденцию, еще больше улучшив результаты за счет возможности обучения более сложных зависимостей и более глубокого представления данных.

Плюсы:
- Лучшая способность запоминания длинных последовательностей.
- Увеличение качества текста при использовании токенизации по словам

Минусы:
- Увеличение вычислительных затрат и времени обучения.

#### Двунаправленная LSTM

Двунаправленная LSTM показала наилучшие результаты среди всех рассмотренных архитектур по метрике перплексии. Благодаря способности учитывать как предыдущий, так и последующий контексты. Но данная архитектура не подходит для генерации текста с 0 или начиная с начального предложения. Двунаправленная LSTM подходит для предсказания слова, если оно находится в середине некоторого контекста.

Плюсы:
- Лучшее качество за счет учета контекста в обеих направлениях.

Минусы:
- Высокая вычислительная сложность и потребность в большом объеме памяти.
- Увеличенное время на обучение и предсказание.
- Неподходящая архитектура для генерации текста с 0 или же последовательно.

### Заключение по RNN

В ходе исследования было показано, что более сложные архитектуры, такие как LSTM, значительно превосходят Simple RNN в задачах генерации текста, особенно при использовании более продвинутых методов токенизации. Однако с увеличением сложности моделей растут и вычислительные затраты, что требует большего количества ресурсов и времени на обучение.

### Трансформерная архитектура (GPT)

В данном пункте необходимо было построить трансформерную архитекутур с нуля. Рассмотрена реализация трансформерной архитектуры Generative Pre-trained Transformer (GPT) с использованием фреймворка PyTorch.

#### Реализация

В основе архитектуры лежит класс GPT, который представляет собой модульную нейронную сеть с использованием трансформерных блоков. Внутри трансформера используются классы для реализации механизма внимания: SingleHead для одной головы и MultiHeadSelfAttention для организации множественного внимания. Эти классы позволяют модели смотреть "назад" и параллельно обработывать разную информацию.

Также использвались векторные представления слов (embeddings), которые обрабатываются и комбинируются перед подачей в модель

#### Обучение модели

Модель обучалась на изначальном датасете, с последующим контролем качества на основании функции потерь и перплексии. Используются динамические паддинги для обработки различных длин входных данных. Обучение модели заняло примерно 2 часа, она обучалась 30 эпох. 

#### Генерация текста

После обучения модель способна генерировать текст, начиная с заданного слова. Из-за того, что модель маленькая, было очень сложно сгенерировать целиком осознанный текст. Получались также и осмысленные высказывания.

#### Заключение
В заключении можно сказать, что трансформерные модели типа GPT демонстрируют более высокую эффективность в задачах генерации текста. Реализация "с нуля" помогает лучше понять механизмы работы таких моделей и дает уникальную возможность адаптации и модификации архитектуры под специфические задачи обработки естественного языка.

### Дообучение предобученной GPT-сети

В этом пункте необходимо было дообучить предобученную GPT-сеть.

#### Этапы реализации проекта

Опять же, используем тот же датасет, что и раньше. Делим его на обучающую и текстовую выборки. Сохраняем преобразованные тексты в обучающий и валидационный файлы. Используем модель  **ai-forever/rugpt3small_based_on_gpt2**, которая адаптирована под русский язык.

Дообучение проводилось на обучающем наборе данных с использованием настроенных параметров. Потери модели уменьшались с каждым шагом обучения.

После завершения обучения была проверена способность модели к генерации текста. Опять же, из-за малости модели текст по промту выдавался невсегда связанный.

#### Основные результаты

- Эффективность дообучения: снижение потерь в процессе обучения указывает на то, что модель успешно адаптировалась к новым данным.
- Качество генерации текста: генерированные моделью тексты были семантически связны и соответствовали заданным промптам.

#### Заключение

Дообучение модели GPT было успешно реализовано, что позволило улучшить качество генерации текстов на русском языке. Предположительно, при увеличении размера модели могли бы получиться более осмысленные выражения.
