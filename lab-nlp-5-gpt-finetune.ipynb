{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2730445,"sourceType":"datasetVersion","datasetId":1167113}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT2 Finetuning on Wikibooks Dataset","metadata":{}},{"cell_type":"markdown","source":"## 1. Загружаем датасет","metadata":{}},{"cell_type":"code","source":"import sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect('/kaggle/input/wikibooks-dataset/wikibooks.sqlite')\n\ndf = pd.read_sql_query(\"SELECT * FROM ru LIMIT 3300\", conn)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:16.235499Z","iopub.execute_input":"2024-05-28T22:18:16.235861Z","iopub.status.idle":"2024-05-28T22:18:18.283613Z","shell.execute_reply.started":"2024-05-28T22:18:16.235813Z","shell.execute_reply":"2024-05-28T22:18:18.282882Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:19.359721Z","iopub.execute_input":"2024-05-28T22:18:19.360098Z","iopub.status.idle":"2024-05-28T22:18:19.381675Z","shell.execute_reply.started":"2024-05-28T22:18:19.360069Z","shell.execute_reply":"2024-05-28T22:18:19.380770Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0  Викиучебник: Техника и технология средств масс...   \n1           Викиучебник: АОН/Пилотское свидетельство   \n2  Викиучебник: Книга программиста/Структуры данн...   \n3  Викиучебник: Тесты НМО/Гигиенические основы и ...   \n4                   Викиучебник: Коктейли/Пенная фея   \n\n                                                 url  \\\n0  https://ru.wikibooks.org/wiki/%D0%A2%D0%B5%D1%...   \n1  https://ru.wikibooks.org/wiki/%D0%90%D0%9E%D0%...   \n2  https://ru.wikibooks.org/wiki/%D0%9A%D0%BD%D0%...   \n3  https://ru.wikibooks.org/wiki/%D0%A2%D0%B5%D1%...   \n4  https://ru.wikibooks.org/wiki/%D0%9A%D0%BE%D0%...   \n\n                                            abstract  \\\n0                       * [станция|Рабочая станция];   \n1  Гражданское пилотское свидетельство - разрешен...   \n2                                       К оглавлению   \n3  Гигиенические основы и медицинский контроль за...   \n4                                         Пенная фея   \n\n                                           body_text  \\\n0  Рабочая станция;\\nСервер;\\nПерсональный компью...   \n1  В Википедии имеется статья по теме «Свидетельс...   \n2  К оглавлению\\nВсе программы, код которых вылож...   \n3  Гигиенические основы и медицинский контроль за...   \n4  Пенная фея\\n\\nДжин Old Tom — 60 г\\nАбсент — 15...   \n\n                                           body_html  \n0  <div class=\"mw-parser-output\"><ul><li><a href=...  \n1  <div class=\"mw-parser-output\"><div class=\"info...  \n2  <div class=\"mw-parser-output\"><p><a href=\"/wik...  \n3  <div class=\"mw-parser-output\"><p><b>Гигиеничес...  \n4  <div class=\"mw-parser-output\"><p><b>Пенная фея...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>url</th>\n      <th>abstract</th>\n      <th>body_text</th>\n      <th>body_html</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Викиучебник: Техника и технология средств масс...</td>\n      <td>https://ru.wikibooks.org/wiki/%D0%A2%D0%B5%D1%...</td>\n      <td>* [станция|Рабочая станция];</td>\n      <td>Рабочая станция;\\nСервер;\\nПерсональный компью...</td>\n      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Викиучебник: АОН/Пилотское свидетельство</td>\n      <td>https://ru.wikibooks.org/wiki/%D0%90%D0%9E%D0%...</td>\n      <td>Гражданское пилотское свидетельство - разрешен...</td>\n      <td>В Википедии имеется статья по теме «Свидетельс...</td>\n      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div class=\"info...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Викиучебник: Книга программиста/Структуры данн...</td>\n      <td>https://ru.wikibooks.org/wiki/%D0%9A%D0%BD%D0%...</td>\n      <td>К оглавлению</td>\n      <td>К оглавлению\\nВсе программы, код которых вылож...</td>\n      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;&lt;a href=\"/wik...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Викиучебник: Тесты НМО/Гигиенические основы и ...</td>\n      <td>https://ru.wikibooks.org/wiki/%D0%A2%D0%B5%D1%...</td>\n      <td>Гигиенические основы и медицинский контроль за...</td>\n      <td>Гигиенические основы и медицинский контроль за...</td>\n      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;&lt;b&gt;Гигиеничес...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Викиучебник: Коктейли/Пенная фея</td>\n      <td>https://ru.wikibooks.org/wiki/%D0%9A%D0%BE%D0%...</td>\n      <td>Пенная фея</td>\n      <td>Пенная фея\\n\\nДжин Old Tom — 60 г\\nАбсент — 15...</td>\n      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;&lt;b&gt;Пенная фея...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df[df['body_text'] != '']","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:19.565782Z","iopub.execute_input":"2024-05-28T22:18:19.566104Z","iopub.status.idle":"2024-05-28T22:18:19.574264Z","shell.execute_reply.started":"2024-05-28T22:18:19.566079Z","shell.execute_reply":"2024-05-28T22:18:19.573366Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 2. Train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, test_texts = train_test_split(df['body_text'], test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:19.923429Z","iopub.execute_input":"2024-05-28T22:18:19.923756Z","iopub.status.idle":"2024-05-28T22:18:20.522648Z","shell.execute_reply.started":"2024-05-28T22:18:19.923729Z","shell.execute_reply":"2024-05-28T22:18:20.521743Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_texts.shape, test_texts.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:20.524460Z","iopub.execute_input":"2024-05-28T22:18:20.524770Z","iopub.status.idle":"2024-05-28T22:18:20.530938Z","shell.execute_reply.started":"2024-05-28T22:18:20.524744Z","shell.execute_reply":"2024-05-28T22:18:20.529872Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((2635,), (659,))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Сохраняем тексты в файлы","metadata":{}},{"cell_type":"code","source":"with open(\"train.txt\", \"w\") as file:\n    file.write(\"\\n\".join(train_texts.tolist()))\n\nwith open(\"valid.txt\", \"w\") as file:\n    file.write(\"\\n\".join(test_texts.tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:20.532410Z","iopub.execute_input":"2024-05-28T22:18:20.532748Z","iopub.status.idle":"2024-05-28T22:18:20.753040Z","shell.execute_reply.started":"2024-05-28T22:18:20.532721Z","shell.execute_reply":"2024-05-28T22:18:20.751868Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 3. Запускаем дообучение","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorForLanguageModeling, TextDataset\nfrom transformers import AdamW, get_cosine_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:36:18.401020Z","iopub.execute_input":"2024-05-29T18:36:18.401378Z","iopub.status.idle":"2024-05-29T18:36:36.090744Z","shell.execute_reply.started":"2024-05-29T18:36:18.401351Z","shell.execute_reply":"2024-05-29T18:36:36.089913Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-29 18:36:27.095505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 18:36:27.095605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 18:36:27.231307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"device = \"cuda\"\n\nmodel_name_or_path = 'ai-forever/rugpt3small_based_on_gpt2'\n\n# tokenizer based on GPT2 for text preprocessing\ntokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n\n# loading a pre-trained model based on GPT2\nmodel = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:37.873398Z","iopub.execute_input":"2024-05-28T22:18:37.873950Z","iopub.status.idle":"2024-05-28T22:18:43.649244Z","shell.execute_reply.started":"2024-05-28T22:18:37.873923Z","shell.execute_reply":"2024-05-28T22:18:43.648230Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27615eb197594464bdd930ffccda5dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6972f0283787472a9eac09f7c75b315a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a46c3665d51f48ff8f06be9bd08708aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eaf0b452fa8464f9de7b6f965dce003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf30243bacc140b6a725e5eb5efce7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf03230322c48cfac638860ddf83d5b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = TextDataset(tokenizer=tokenizer,file_path='/kaggle/working/train.txt', \n                            block_size=64)\n  \ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n                                                mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:43.650568Z","iopub.execute_input":"2024-05-28T22:18:43.650882Z","iopub.status.idle":"2024-05-28T22:18:44.254488Z","shell.execute_reply.started":"2024-05-28T22:18:43.650851Z","shell.execute_reply":"2024-05-28T22:18:44.253527Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = \"./finetuned_model\",\n    overwrite_output_dir = True,\n    num_train_epochs = 10,\n    gradient_accumulation_steps = 2,\n    fp16 = True,\n    per_device_train_batch_size = 64,\n    learning_rate = 0.0002,\n    optim = 'adafactor',\n    lr_scheduler_type = 'cosine',\n    save_steps=1000,\n    seed=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:44.256442Z","iopub.execute_input":"2024-05-28T22:18:44.256740Z","iopub.status.idle":"2024-05-28T22:18:44.284361Z","shell.execute_reply.started":"2024-05-28T22:18:44.256714Z","shell.execute_reply":"2024-05-28T22:18:44.283472Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:44.285542Z","iopub.execute_input":"2024-05-28T22:18:44.285899Z","iopub.status.idle":"2024-05-28T22:18:44.933286Z","shell.execute_reply.started":"2024-05-28T22:18:44.285864Z","shell.execute_reply":"2024-05-28T22:18:44.932335Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf wandb & rm -rf finetuned_model","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:44.934543Z","iopub.execute_input":"2024-05-28T22:18:44.934830Z","iopub.status.idle":"2024-05-28T22:18:45.928497Z","shell.execute_reply.started":"2024-05-28T22:18:44.934794Z","shell.execute_reply":"2024-05-28T22:18:45.927310Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T22:18:45.930053Z","iopub.execute_input":"2024-05-28T22:18:45.930384Z","iopub.status.idle":"2024-05-29T00:10:29.098619Z","shell.execute_reply.started":"2024-05-28T22:18:45.930354Z","shell.execute_reply":"2024-05-29T00:10:29.097770Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240528_221900-9uf502zt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/l-rekhlov-inno/huggingface/runs/9uf502zt' target=\"_blank\">apricot-star-6</a></strong> to <a href='https://wandb.ai/l-rekhlov-inno/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/l-rekhlov-inno/huggingface' target=\"_blank\">https://wandb.ai/l-rekhlov-inno/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/l-rekhlov-inno/huggingface/runs/9uf502zt' target=\"_blank\">https://wandb.ai/l-rekhlov-inno/huggingface/runs/9uf502zt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5860' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5860/5860 1:51:08, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.958200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.520200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.274900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.087500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.918200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.768100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.632600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.506500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.431100</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.380700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.347800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5860, training_loss=1.8585257103825592, metrics={'train_runtime': 6702.7479, 'train_samples_per_second': 111.953, 'train_steps_per_second': 0.874, 'total_flos': 2.4489040453632e+16, 'train_loss': 1.8585257103825592, 'epoch': 9.99})"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. Генерируем примеры текста","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef generate(prompt, do_sample=True, num_beams=2, temperature=1.5, top_p=0.9, max_length=75):\n    \n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    model.eval()\n    with torch.no_grad():\n        out = model.generate(input_ids, \n                            do_sample=do_sample,\n                            num_beams=num_beams,\n                            temperature=temperature,\n                            top_p=top_p,\n                            max_length=max_length,\n                            )\n\n    print(list(map(tokenizer.decode, out))[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:43:42.446188Z","iopub.execute_input":"2024-05-29T18:43:42.447034Z","iopub.status.idle":"2024-05-29T18:43:42.453348Z","shell.execute_reply.started":"2024-05-29T18:43:42.447002Z","shell.execute_reply":"2024-05-29T18:43:42.452337Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:39:24.149359Z","iopub.execute_input":"2024-05-29T18:39:24.150001Z","iopub.status.idle":"2024-05-29T18:39:24.154488Z","shell.execute_reply.started":"2024-05-29T18:39:24.149971Z","shell.execute_reply":"2024-05-29T18:39:24.153416Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(\"./finetuned_model/checkpoint-5000\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:42:26.590336Z","iopub.execute_input":"2024-05-29T18:42:26.591100Z","iopub.status.idle":"2024-05-29T18:42:26.597539Z","shell.execute_reply.started":"2024-05-29T18:42:26.591069Z","shell.execute_reply":"2024-05-29T18:42:26.596509Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\n\nmodel_name_or_path = 'ai-forever/rugpt3small_based_on_gpt2'\n\n# tokenizer based on GPT2 for text preprocessing\ntokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n\n# loading a pre-trained model based on GPT2\nmodel = GPT2LMHeadModel(config=config).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:42:48.277420Z","iopub.execute_input":"2024-05-29T18:42:48.277843Z","iopub.status.idle":"2024-05-29T18:42:53.382914Z","shell.execute_reply.started":"2024-05-29T18:42:48.277813Z","shell.execute_reply":"2024-05-29T18:42:53.382062Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f8328880814c0080da2a11895bef1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bc53d8c3c441ed9e5686dfcb59fa0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9283fbcbcf534a1fa807814c33a4a5fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8114d59fe97a445f849ff96b47d379e2"}},"metadata":{}}]},{"cell_type":"code","source":"generate(\"женщина\", max_length=30)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:47:22.022204Z","iopub.execute_input":"2024-05-29T18:47:22.022741Z","iopub.status.idle":"2024-05-29T18:47:22.385998Z","shell.execute_reply.started":"2024-05-29T18:47:22.022710Z","shell.execute_reply":"2024-05-29T18:47:22.384931Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"женщина тык✂✂ потерять предательство назначения получимварда 123ike геопол геополгрегрегре отдельными 123 проведено путями полиэти контур 123 123 лезетшней Ставрополь Ставропольota\n","output_type":"stream"}]}]}